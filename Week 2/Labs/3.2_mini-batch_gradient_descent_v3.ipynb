{"cells":[{"cell_type":"markdown","id":"6225b7ef-af0e-46f4-be7f-eb5408405727","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"5a382fdb-7112-44b7-80b3-d18f0b416f37","metadata":{},"outputs":[],"source":["\u003ch1\u003eLinear Regression 1D: Training Two Parameter Mini-Batch Gradient Decent\u003c/h1\u003e\n"]},{"cell_type":"markdown","id":"96a60d0f-ed28-47f8-9964-57d3ddd2cce0","metadata":{},"outputs":[],"source":["\u003ch2\u003eObjective\u003c/h2\u003e\u003cul\u003e\u003cli\u003e How to use Mini-Batch Gradient Descent to train model.\u003c/li\u003e\u003c/ul\u003e \n"]},{"cell_type":"markdown","id":"2604a64a-dcac-468c-8c83-63ecbd475429","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003cp\u003eIn this Lab, you will practice training a model by using Mini-Batch Gradient Descent.\u003c/p\u003e\n","\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#Makeup_Data\"\u003eMake Some Data\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Model_Cost\"\u003eCreate the Model and Cost Function (Total Loss)\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#BGD\"\u003eTrain the Model: Batch Gradient Descent\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#SGD\"\u003eTrain the Model: Stochastic Gradient Descent with Dataset DataLoader\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Mini5\"\u003eTrain the Model: Mini Batch Gradient Decent: Batch Size Equals 5\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Mini10\"\u003eTrain the Model: Mini Batch Gradient Decent: Batch Size Equals 10\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e30 min\u003c/strong\u003e\u003c/p\u003e\n","\u003c/div\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"8d60ef83-4bbc-4563-83f5-2382303243e0","metadata":{},"outputs":[],"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"8b7c39fb-b841-49e6-ac97-2dab18ba551a","metadata":{},"outputs":[],"source":["We'll need the following libraries:\n"]},{"cell_type":"code","id":"7e669162-f246-4ddd-9fb2-eff719ed7c5e","metadata":{},"outputs":[],"source":["# Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d"]},{"cell_type":"markdown","id":"96ecafe1-adcd-4024-afcd-7a4e5c4f09e1","metadata":{},"outputs":[],"source":["The class \u003ccode\u003eplot_error_surfaces\u003c/code\u003e is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch. \n"]},{"cell_type":"code","id":"6ff37e9d-cf16-4a2c-acfe-715c9e5811ec","metadata":{},"outputs":[],"source":["# The class for plotting the diagrams\n\nclass plot_error_surfaces(object):\n    \n    # Constructor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n                count2 += 1\n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize = (7.5, 5))\n            plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1, cmap = 'viridis', edgecolor = 'none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n            \n     # Setter\n    def set_para_loss(self, W, B, loss):\n        self.n = self.n + 1\n        self.W.append(W)\n        self.B.append(B)\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection = '3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n    \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim()\n        plt.plot(self.x, self.y, 'ro', label = \"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.title('Data Space Iteration: '+ str(self.n))\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.title('Loss Surface Contour')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()"]},{"cell_type":"markdown","id":"dfb20107-28e3-4153-bad0-5cc34c6f2834","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"3cecc2ed-d3cd-4ed0-9b05-c139439ffda6","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Makeup_Data\"\u003eMake Some Data \u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"bffc7655-302b-460f-9844-53c0c9085e19","metadata":{},"outputs":[],"source":["Import PyTorch and set random seed:\n"]},{"cell_type":"code","id":"d74a500a-3183-4091-ba7d-72962057c308","metadata":{},"outputs":[],"source":["# Import PyTorch library\n\nimport torch\ntorch.manual_seed(1)"]},{"cell_type":"markdown","id":"fcf3b962-dd84-448f-bf8d-a284da612099","metadata":{},"outputs":[],"source":["Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:\n"]},{"cell_type":"code","id":"720c8f7d-ea68-45d6-a762-0eb85be41c93","metadata":{},"outputs":[],"source":["# Generate the data with noise and the line\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\nY = f + 0.1 * torch.randn(X.size())"]},{"cell_type":"markdown","id":"1e722603-3ff1-4ff6-94e3-d48b6cab52a4","metadata":{},"outputs":[],"source":["Plot the results:\n"]},{"cell_type":"code","id":"3b09b310-2d1b-4057-8627-2a78633073cb","metadata":{},"outputs":[],"source":["# Plot the line and the data\n\nplt.plot(X.numpy(), Y.numpy(), 'rx', label = 'y')\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"806ae325-a18e-4140-aea8-b0887cf50d34","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"24d96781-cba9-465e-9974-ec0437202320","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Model_Cost\"\u003eCreate the Model and Cost Function (Total Loss) \u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"639aa141-cde0-40f7-aad0-f1676f0995e9","metadata":{},"outputs":[],"source":["Define the \u003ccode\u003eforward\u003c/code\u003e function: \n"]},{"cell_type":"code","id":"b09f8b60-cd1b-475a-8cf7-b508d12a1f77","metadata":{},"outputs":[],"source":["# Define the prediction function\n\ndef forward(x):\n    return w * x + b"]},{"cell_type":"markdown","id":"87ca2113-1f4e-4010-8995-d086171a9031","metadata":{},"outputs":[],"source":["Define the cost or criterion function: \n"]},{"cell_type":"code","id":"4597fd09-39fc-4f7f-8944-0b16c06a7ee2","metadata":{},"outputs":[],"source":["# Define the cost function\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)"]},{"cell_type":"markdown","id":"62d1a7f8-e97a-4138-9ae5-0f9eb5103a27","metadata":{},"outputs":[],"source":["Create a \u003ccode\u003e plot_error_surfaces\u003c/code\u003e object to visualize the data space and the parameter space during training:\n"]},{"cell_type":"code","id":"30ad13f7-bead-412a-bd04-8457d8b547cf","metadata":{},"outputs":[],"source":["# Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30)"]},{"cell_type":"markdown","id":"f5b8f03b-4796-4e9c-8adb-6a22d4b83113","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"1775a650-418f-4732-b65e-6fa1dcbe8299","metadata":{},"outputs":[],"source":["\u003ch2\u003eTrain the Model: Batch Gradient Descent (BGD)\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"496206ac-50f9-4e77-a68b-f7651fa8d931","metadata":{},"outputs":[],"source":["Define \u003ccode\u003etrain_model_BGD\u003c/code\u003e function.\n"]},{"cell_type":"code","id":"2c7c9fd6-02d9-47af-a63f-129d64000467","metadata":{},"outputs":[],"source":["# Define the function for training model\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nlr = 0.1\nLOSS_BGD = []\n\ndef train_model_BGD(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        loss = criterion(Yhat, Y)\n        LOSS_BGD.append(loss)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        get_surface.plot_ps()\n        loss.backward()\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()"]},{"cell_type":"markdown","id":"aca605ee-3ae4-4e93-90c7-0dc7c31858b4","metadata":{},"outputs":[],"source":["Run 10 epochs of batch gradient descent: \u003cb\u003ebug\u003c/b\u003e data space is 1 iteration ahead of parameter space. \n"]},{"cell_type":"code","id":"cca4ba1a-b765-4d57-8858-ed8c5787b369","metadata":{},"outputs":[],"source":["# Run train_model_BGD with 10 iterations\n\ntrain_model_BGD(10)"]},{"cell_type":"markdown","id":"f6f7388d-dcf1-4038-923f-dc0858c2e402","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"55c3e7e1-776f-4df4-a027-d55ba318799d","metadata":{},"outputs":[],"source":["\u003ch2 id=\"SGD\"\u003e Stochastic Gradient Descent (SGD) with Dataset DataLoader\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"270115f5-6ba3-4107-a5db-65651f78322e","metadata":{},"outputs":[],"source":["Create a \u003ccode\u003eplot_error_surfaces\u003c/code\u003e object to visualize the data space and the parameter space during training:\n"]},{"cell_type":"code","id":"6947837f-90b0-40f0-86e1-960e903dcdcc","metadata":{},"outputs":[],"source":["# Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)"]},{"cell_type":"markdown","id":"976b0262-53bf-4d15-b411-87cb83cb5d2f","metadata":{},"outputs":[],"source":["Import \u003ccode\u003eDataset\u003c/code\u003e and \u003ccode\u003eDataLoader\u003c/code\u003e libraries\n"]},{"cell_type":"code","id":"2905fcc2-16ba-449d-8606-4d791081ad31","metadata":{},"outputs":[],"source":["# Import libraries\n\nfrom torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","id":"891e553e-eb81-4c1b-b926-13bd22ceea80","metadata":{},"outputs":[],"source":["Create \u003ccode\u003eData\u003c/code\u003e class\n"]},{"cell_type":"code","id":"38c56908-ea80-4a2c-ba0b-41d4ccd68a02","metadata":{},"outputs":[],"source":["# Create class Data\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.y = 1 * X - 1\n        self.len = self.x.shape[0]\n        \n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n    \n    # Get length\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"dac0b63c-f92e-42ef-a5fc-c8c0b3e17ca2","metadata":{},"outputs":[],"source":["Create a dataset object and a dataloader object: \n"]},{"cell_type":"code","id":"ec2093f1-2dd4-482d-b7cb-4ea35e1725ff","metadata":{},"outputs":[],"source":["# Create Data object and DataLoader object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)"]},{"cell_type":"markdown","id":"d989faf5-12dc-4f2d-adac-69031171c42d","metadata":{},"outputs":[],"source":["Define \u003ccode\u003etrain_model_SGD\u003c/code\u003e function for training the model.\n"]},{"cell_type":"code","id":"4a757e7e-0518-425f-80ec-fc575dffa2ff","metadata":{},"outputs":[],"source":["# Define train_model_SGD function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_SGD = []\nlr = 0.1\ndef train_model_SGD(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_SGD.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"18bc65b5-15b6-41c0-a533-416901c67a73","metadata":{},"outputs":[],"source":["Run 10 epochs of stochastic gradient descent: \u003cb\u003ebug\u003c/b\u003e data space is 1 iteration ahead of parameter space. \n"]},{"cell_type":"code","id":"5ee9e42c-15bb-45b4-bd6a-f8cda5278622","metadata":{},"outputs":[],"source":["# Run train_model_SGD(iter) with 10 iterations\n\ntrain_model_SGD(10)"]},{"cell_type":"markdown","id":"88227eb1-101e-4ca6-88a4-4c055c3e12ef","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"7985d533-9a20-4753-b1f1-ca0a0b77a397","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Mini5\"\u003eMini Batch Gradient Descent: Batch Size Equals 5\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"a28de03b-88f4-4d60-8be9-45b5fc681149","metadata":{},"outputs":[],"source":["Create a \u003ccode\u003e plot_error_surfaces\u003c/code\u003e object to visualize the data space and the parameter space during training:\n"]},{"cell_type":"code","id":"a0be22e8-1a80-4f71-a37f-e6a1826c15a8","metadata":{},"outputs":[],"source":["# Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)"]},{"cell_type":"markdown","id":"d60750f1-19bb-4799-ac4e-5030b3710ab3","metadata":{},"outputs":[],"source":["Create \u003ccode\u003eData\u003c/code\u003e object and create a \u003ccode\u003eDataloader\u003c/code\u003e object where the batch size equals 5:\n"]},{"cell_type":"code","id":"74549c99-e229-4048-8af7-90901ac95a89","metadata":{},"outputs":[],"source":["# Create DataLoader object and Data object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 5)"]},{"cell_type":"markdown","id":"7944a259-c2fd-4283-add6-872cfcaa6242","metadata":{},"outputs":[],"source":["Define \u003ccode\u003etrain_model_Mini5\u003c/code\u003e function to train the model.\n"]},{"cell_type":"code","id":"0bec559e-6420-4d66-99b4-a7906d358310","metadata":{},"outputs":[],"source":["# Define train_model_Mini5 function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_MINI5 = []\nlr = 0.1\n\ndef train_model_Mini5(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI5.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()"]},{"cell_type":"markdown","id":"66964b63-427d-4c72-8a27-0d133e03c94b","metadata":{},"outputs":[],"source":["Run 10 epochs of mini-batch gradient descent: \u003cb\u003ebug\u003c/b\u003e data space is 1 iteration ahead of parameter space. \n"]},{"cell_type":"code","id":"20f8e722-9795-4789-a9e1-14a89b754e4b","metadata":{},"outputs":[],"source":["# Run train_model_Mini5 with 10 iterations.\n\ntrain_model_Mini5(10)"]},{"cell_type":"markdown","id":"3d2d7f53-b183-442e-aca7-cf8fa1c4ef7d","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"670f6fac-a02e-444e-a2c4-a3e73812e700","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Mini10\"\u003eMini Batch Gradient Descent: Batch Size Equals 10\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"2907fc47-11d1-48f1-85a5-c96820ac58e1","metadata":{},"outputs":[],"source":["Create a \u003ccode\u003e plot_error_surfaces\u003c/code\u003e object to visualize the data space and the parameter space during training:\n"]},{"cell_type":"code","id":"b069cf61-072b-44c3-811a-e10724162e96","metadata":{},"outputs":[],"source":["# Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)"]},{"cell_type":"markdown","id":"540f0748-819e-4a15-ac79-cf372b73c290","metadata":{},"outputs":[],"source":["Create \u003ccode\u003eData\u003c/code\u003e object and create a \u003ccode\u003eDataloader\u003c/code\u003e object batch size equals 10\n"]},{"cell_type":"code","id":"478190eb-40c5-475b-8c1d-4a8750b15bf3","metadata":{},"outputs":[],"source":["# Create DataLoader object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 10)"]},{"cell_type":"markdown","id":"832d7ade-808e-48c5-893e-6da26d8e911d","metadata":{},"outputs":[],"source":["Define \u003ccode\u003etrain_model_Mini10\u003c/code\u003e function for training the model.\n"]},{"cell_type":"code","id":"31555559-7ee7-44c9-90fa-07265c065536","metadata":{},"outputs":[],"source":["# Define train_model_Mini5 function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_MINI10 = []\nlr = 0.1\n\ndef train_model_Mini10(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI10.append(criterion(forward(X),Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()"]},{"cell_type":"markdown","id":"d20016c7-b7af-4b43-ba9f-9172d8e2d435","metadata":{},"outputs":[],"source":["Run 10 epochs of mini-batch gradient descent: \u003cb\u003ebug\u003c/b\u003e data space is 1 iteration ahead of parameter space. \n"]},{"cell_type":"code","id":"1547d313-b850-418e-9cd0-e304e3f7ba00","metadata":{},"outputs":[],"source":["# Run train_model_Mini5 with 10 iterations.\n\ntrain_model_Mini10(10)"]},{"cell_type":"markdown","id":"480945af-18f0-4901-9f0b-1fcaec8dc96b","metadata":{},"outputs":[],"source":["Plot the loss for each epoch:  \n"]},{"cell_type":"code","id":"2c286849-741e-4fca-b8c4-ac58b8338a1d","metadata":{},"outputs":[],"source":["# Plot out the LOSS for each method\n\nplt.plot(LOSS_BGD,label = \"Batch Gradient Descent\")\nplt.plot(LOSS_SGD,label = \"Stochastic Gradient Descent\")\nplt.plot(LOSS_MINI5,label = \"Mini-Batch Gradient Descent, Batch size: 5\")\nplt.plot(LOSS_MINI10,label = \"Mini-Batch Gradient Descent, Batch size: 10\")\nplt.legend()"]},{"cell_type":"markdown","id":"cf489493-0d7b-4453-9903-f75c51982dba","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"45de039e-910a-48dd-a391-fce1040a404f","metadata":{},"outputs":[],"source":["\u003ch3\u003ePractice\u003c/h3\u003e\n"]},{"cell_type":"markdown","id":"8f36e7f0-b7d9-4d03-82d1-b6a94bb8fee9","metadata":{},"outputs":[],"source":["Perform mini batch gradient descent with a batch size of 20. Store the total loss for each epoch in the list LOSS20.  \n"]},{"cell_type":"code","id":"f722103a-a127-4662-a192-e9df696ca9fa","metadata":{},"outputs":[],"source":["# Practice: Perform mini batch gradient descent with a batch size of 20.\n\ndataset = Data()"]},{"cell_type":"markdown","id":"a67e4689-7e1e-4cf3-bfd1-34ff545cd0ad","metadata":{},"outputs":[],"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\n","\u003c!-- \n","trainloader = DataLoader(dataset = dataset, batch_size = 20)\n","w = torch.tensor(-15.0, requires_grad = True)\n","b = torch.tensor(-10.0, requires_grad = True)\n","\n","LOSS_MINI20 = []\n","lr = 0.1\n","\n","def my_train_model(epochs):\n","    for epoch in range(epochs):\n","        Yhat = forward(X)\n","        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n","        get_surface.plot_ps()\n","        LOSS_MINI20.append(criterion(forward(X), Y).tolist())\n","        for x, y in trainloader:\n","            yhat = forward(x)\n","            loss = criterion(yhat, y)\n","            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n","            loss.backward()\n","            w.data = w.data - lr * w.grad.data\n","            b.data = b.data - lr * b.grad.data\n","            w.grad.data.zero_()\n","            b.grad.data.zero_()\n","\n","my_train_model(10)\n","--\u003e\n"]},{"cell_type":"markdown","id":"c3442d16-8bbc-4bd9-adf8-e19bd5012218","metadata":{},"outputs":[],"source":["Plot a graph that shows the LOSS results for all the methods.\n"]},{"cell_type":"code","id":"d25b983c-b18f-43e0-b2a7-82da454bd37e","metadata":{},"outputs":[],"source":["# Practice: Plot a graph to show all the LOSS functions\n\n# Type your code here"]},{"cell_type":"markdown","id":"afb0a41b-1542-419c-8c5b-47b94ab8e139","metadata":{},"outputs":[],"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\n","\u003c!-- \n","plt.plot(LOSS_BGD, label = \"Batch Gradient Descent\")\n","plt.plot(LOSS_SGD, label = \"Stochastic Gradient Descent\")\n","plt.plot(LOSS_MINI5, label = \"Mini-Batch Gradient Descent,Batch size:5\")\n","plt.plot(LOSS_MINI10, label = \"Mini-Batch Gradient Descent,Batch size:10\")\n","plt.plot(LOSS_MINI20, label = \"Mini-Batch Gradient Descent,Batch size:20\")\n","plt.legend()\n","--\u003e\n"]},{"cell_type":"markdown","id":"6d92ac6e-212d-44ee-8673-f5893926a25a","metadata":{},"outputs":[],"source":["\n","\n","\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"379ea3d0-bfe9-4662-afcf-7a655af2a91f","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"e194f0b6-ddc1-477f-82b7-effb5b45bf81","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD. \n"]},{"cell_type":"markdown","id":"24fc4c25-a02d-413d-9ff3-c75c6d67baa5","metadata":{},"outputs":[],"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"414ea8cf-49d1-4bb6-b2c4-269ca322d2fa","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n"]},{"cell_type":"markdown","id":"94e51642-b9be-4f0d-b840-27508784183c","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"73df5064-a8d4-4e92-87a9-3b57a6f9ec1d","metadata":{},"outputs":[],"source":["\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2020. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}